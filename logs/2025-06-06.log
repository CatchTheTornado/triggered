2025-06-06 14:14:20,890 - LiteLLM - DEBUG - 

2025-06-06 14:14:20,891 - LiteLLM - DEBUG - [92mRequest to litellm:[0m
2025-06-06 14:14:20,891 - LiteLLM - DEBUG - [92mlitellm.completion(tools=[{'type': 'function', 'function': {'name': 'random_number', 'description': 'Generate a random number between min_value and max_value (inclusive)', 'parameters': {'description': 'Input schema for RandomNumberTool.', 'properties': {'min_value': {'default': 1, 'description': 'Minimum value (inclusive)', 'title': 'Min Value', 'type': 'integer'}, 'max_value': {'default': 10, 'description': 'Maximum value (inclusive)', 'title': 'Max Value', 'type': 'integer'}}, 'title': 'RandomNumberInput', 'type': 'object'}}}], model='ollama/llama3.1', messages=[{'role': 'system', 'content': 'You have access to the following tools:', 'tools': [{'type': 'function', 'function': {'name': 'random_number', 'description': 'Generate a random number between min_value and max_value (inclusive)', 'parameters': {'description': 'Input schema for RandomNumberTool.', 'properties': {'min_value': {'default': 1, 'description': 'Minimum value (inclusive)', 'title': 'Min Value', 'type': 'integer'}, 'max_value': {'default': 10, 'description': 'Maximum value (inclusive)', 'title': 'Max Value', 'type': 'integer'}}, 'title': 'RandomNumberInput', 'type': 'object'}}}]}, {'role': 'user', 'content': 'You are the decision maker if to run the user action or not. You must return a JSON response with the following schema: { "trigger": <true|false>, "reason": "<short explanation why you made the decision>" }. Here is the user defined criteria for you to consider:\n\nGenerate a random number between 1 and 10. Make the decision based on the number - if >=5 then trigger otherwise don\'t trigger'}], api_base='http://localhost:11434')[0m
2025-06-06 14:14:20,891 - LiteLLM - DEBUG - 

2025-06-06 14:14:20,891 - LiteLLM - DEBUG - self.optional_params: {}
2025-06-06 14:14:20,891 - LiteLLM - DEBUG - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False
2025-06-06 14:14:20,893 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.1; provider = ollama
2025-06-06 14:14:20,894 - LiteLLM - DEBUG - 
LiteLLM: Params passed to completion() {'model': 'llama3.1', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'ollama', 'response_format': None, 'seed': None, 'tools': [{'type': 'function', 'function': {'name': 'random_number', 'description': 'Generate a random number between min_value and max_value (inclusive)', 'parameters': {'description': 'Input schema for RandomNumberTool.', 'properties': {'min_value': {'default': 1, 'description': 'Minimum value (inclusive)', 'title': 'Min Value', 'type': 'integer'}, 'max_value': {'default': 10, 'description': 'Maximum value (inclusive)', 'title': 'Max Value', 'type': 'integer'}}, 'title': 'RandomNumberInput', 'type': 'object'}}}], 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'You have access to the following tools:', 'tools': [{'type': 'function', 'function': {'name': 'random_number', 'description': 'Generate a random number between min_value and max_value (inclusive)', 'parameters': {'description': 'Input schema for RandomNumberTool.', 'properties': {'min_value': {'default': 1, 'description': 'Minimum value (inclusive)', 'title': 'Min Value', 'type': 'integer'}, 'max_value': {'default': 10, 'description': 'Maximum value (inclusive)', 'title': 'Max Value', 'type': 'integer'}}, 'title': 'RandomNumberInput', 'type': 'object'}}}]}, {'role': 'user', 'content': 'You are the decision maker if to run the user action or not. You must return a JSON response with the following schema: { "trigger": <true|false>, "reason": "<short explanation why you made the decision>" }. Here is the user defined criteria for you to consider:\n\nGenerate a random number between 1 and 10. Make the decision based on the number - if >=5 then trigger otherwise don\'t trigger'}], 'thinking': None, 'web_search_options': None}
2025-06-06 14:14:20,894 - LiteLLM - DEBUG - 
LiteLLM: Non-Default params passed to completion() {}
2025-06-06 14:14:20,894 - LiteLLM - DEBUG - Final returned optional params: {'format': 'json', 'functions_unsupported_model': [{'type': 'function', 'function': {'name': 'random_number', 'description': 'Generate a random number between min_value and max_value (inclusive)', 'parameters': {'description': 'Input schema for RandomNumberTool.', 'properties': {'min_value': {'default': 1, 'description': 'Minimum value (inclusive)', 'title': 'Min Value', 'type': 'integer'}, 'max_value': {'default': 10, 'description': 'Maximum value (inclusive)', 'title': 'Max Value', 'type': 'integer'}}, 'title': 'RandomNumberInput', 'type': 'object'}}}]}
2025-06-06 14:14:20,894 - LiteLLM - DEBUG - self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'random_number', 'description': 'Generate a random number between min_value and max_value (inclusive)', 'parameters': {'description': 'Input schema for RandomNumberTool.', 'properties': {'min_value': {'default': 1, 'description': 'Minimum value (inclusive)', 'title': 'Min Value', 'type': 'integer'}, 'max_value': {'default': 10, 'description': 'Maximum value (inclusive)', 'title': 'Max Value', 'type': 'integer'}}, 'title': 'RandomNumberInput', 'type': 'object'}}}]}
2025-06-06 14:14:20,894 - LiteLLM - DEBUG - [92m

POST Request Sent from LiteLLM:
curl -X POST \
http://localhost:11434/api/generate \
-d '{'model': 'llama3.1', 'prompt': '### System:\nYou have access to the following tools: Produce JSON OUTPUT ONLY! Adhere to this format {"name": "function_name", "arguments":{"argument_name": "argument_value"}} The following functions are available to you:\n{\'type\': \'function\', \'function\': {\'name\': \'random_number\', \'description\': \'Generate a random number between min_value and max_value (inclusive)\', \'parameters\': {\'description\': \'Input schema for RandomNumberTool.\', \'properties\': {\'min_value\': {\'default\': 1, \'description\': \'Minimum value (inclusive)\', \'title\': \'Min Value\', \'type\': \'integer\'}, \'max_value\': {\'default\': 10, \'description\': \'Maximum value (inclusive)\', \'title\': \'Max Value\', \'type\': \'integer\'}}, \'title\': \'RandomNumberInput\', \'type\': \'object\'}}}\n\n\n### User:\nYou are the decision maker if to run the user action or not. You must return a JSON response with the following schema: { "trigger": <true|false>, "reason": "<short explanation why you made the decision>" }. Here is the user defined criteria for you to consider:\n\nGenerate a random number between 1 and 10. Make the decision based on the number - if >=5 then trigger otherwise don\'t trigger\n\n', 'options': {}, 'stream': False, 'format': 'json', 'images': []}'
[0m

2025-06-06 14:14:20,894 - httpx - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False
2025-06-06 14:14:20,895 - httpx - DEBUG - load_verify_locations cafile='/Users/piotrkarwatka/Library/Caches/pypoetry/virtualenvs/triggered-FMy3JBFr-py3.13/lib/python3.13/site-packages/certifi/cacert.pem'
2025-06-06 14:14:20,899 - httpcore.connection - DEBUG - connect_tcp.started host='localhost' port=11434 local_address=None timeout=600.0 socket_options=None
2025-06-06 14:14:20,900 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1281b2e90>
2025-06-06 14:14:20,900 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-06 14:14:20,900 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-06 14:14:20,900 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-06 14:14:20,900 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-06 14:14:20,900 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-06 14:14:22,028 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Fri, 06 Jun 2025 11:14:22 GMT'), (b'Content-Length', b'1657')])
2025-06-06 14:14:22,029 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-06 14:14:22,030 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-06 14:14:22,031 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-06 14:14:22,031 - httpcore.http11 - DEBUG - response_closed.started
2025-06-06 14:14:22,031 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-06 14:14:22,033 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-06-06 14:14:22,035 - LiteLLM - DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
2025-06-06 14:14:22,036 - LiteLLM - INFO - selected model name for cost calculation: ollama/llama3.1
2025-06-06 14:14:22,036 - LiteLLM - INFO - selected model name for cost calculation: ollama/llama3.1
2025-06-06 14:14:22,037 - LiteLLM - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'llama3.1', 'combined_model_name': 'ollama/llama3.1', 'stripped_model_name': 'llama3.1', 'combined_stripped_model_name': 'ollama/llama3.1', 'custom_llm_provider': 'ollama'}
2025-06-06 14:14:22,037 - LiteLLM - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'llama3.1', 'combined_model_name': 'ollama/llama3.1', 'stripped_model_name': 'llama3.1', 'combined_stripped_model_name': 'ollama/llama3.1', 'custom_llm_provider': 'ollama'}
2025-06-06 14:14:22,037 - LiteLLM - DEBUG - Returned custom cost for model=ollama/llama3.1 - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0
2025-06-06 14:14:22,037 - LiteLLM - DEBUG - Returned custom cost for model=ollama/llama3.1 - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0
2025-06-06 14:14:22,038 - LiteLLM - DEBUG - response_cost: 0.0
2025-06-06 14:14:22,038 - LiteLLM - DEBUG - response_cost: 0.0
2025-06-06 14:14:22,039 - LiteLLM - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'llama3.1', 'combined_model_name': 'ollama/llama3.1', 'stripped_model_name': 'llama3.1', 'combined_stripped_model_name': 'ollama/llama3.1', 'custom_llm_provider': 'ollama'}
2025-06-06 14:14:22,040 - LiteLLM - DEBUG - model_info: {'key': 'ollama/llama3.1', 'max_tokens': 32768, 'max_input_tokens': 8192, 'max_output_tokens': 8192, 'input_cost_per_token': 0.0, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0.0, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'ollama', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': None, 'supports_vision': None, 'supports_function_calling': True, 'supports_tool_choice': None, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}
2025-06-06 14:14:22,041 - LiteLLM - DEBUG - Logging Details LiteLLM-Success Call streaming complete
2025-06-06 14:14:22,041 - LiteLLM - INFO - selected model name for cost calculation: ollama/llama3.1
2025-06-06 14:14:22,043 - LiteLLM - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'llama3.1', 'combined_model_name': 'ollama/llama3.1', 'stripped_model_name': 'llama3.1', 'combined_stripped_model_name': 'ollama/llama3.1', 'custom_llm_provider': 'ollama'}
2025-06-06 14:14:22,043 - LiteLLM - DEBUG - Returned custom cost for model=ollama/llama3.1 - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0
2025-06-06 14:14:22,043 - LiteLLM - DEBUG - response_cost: 0.0
2025-06-06 14:14:22,045 - LiteLLM - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'llama3.1', 'combined_model_name': 'ollama/llama3.1', 'stripped_model_name': 'llama3.1', 'combined_stripped_model_name': 'ollama/llama3.1', 'custom_llm_provider': 'ollama'}
2025-06-06 14:14:22,056 - LiteLLM - DEBUG - model_info: {'key': 'ollama/llama3.1', 'max_tokens': 32768, 'max_input_tokens': 8192, 'max_output_tokens': 8192, 'input_cost_per_token': 0.0, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 0.0, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'ollama', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': None, 'supports_vision': None, 'supports_function_calling': True, 'supports_tool_choice': None, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}
2025-06-06 14:14:22,062 - celery.utils.functional - DEBUG - 
def execute_action(ta_dict, ctx_dict):
    return 1

2025-06-06 14:14:22,073 - kombu.connection - WARNING - No hostname was supplied. Reverting to default 'localhost'
2025-06-06 14:15:09,259 - httpcore.connection - DEBUG - close.started
2025-06-06 14:15:09,259 - httpcore.connection - DEBUG - close.complete
2025-06-06 14:17:38,947 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.1; provider = ollama
2025-06-06 14:17:43,678 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-06 14:17:43,681 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-06-06 14:17:43,682 - LiteLLM - INFO - selected model name for cost calculation: ollama/llama3.1
2025-06-06 14:17:43,682 - LiteLLM - INFO - selected model name for cost calculation: ollama/llama3.1
2025-06-06 14:17:43,683 - LiteLLM - INFO - selected model name for cost calculation: ollama/llama3.1
2025-06-06 14:17:43,684 - triggered.models - ERROR - No content in LiteLLM response: ModelResponse(id='chatcmpl-ee8cbde9-5a9d-4a92-a37a-7fa801546efe', created=1749208663, model='ollama/llama3.1', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{"min_value": "1", "max_value": "10"}', name='random_number'), id='call_556d66c7-1d52-4150-8be4-ab16311125f5', type='function')], function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=0, prompt_tokens=266, total_tokens=266, completion_tokens_details=None, prompt_tokens_details=None))
2025-06-06 14:17:43,686 - triggered.triggers.ai - ERROR - Failed to parse model response as JSON: Error: Empty response from model
2025-06-06 14:22:16,726 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.1; provider = ollama
2025-06-06 14:22:20,428 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-06 14:22:20,431 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-06-06 14:22:20,433 - LiteLLM - INFO - selected model name for cost calculation: ollama/llama3.1
2025-06-06 14:22:20,433 - LiteLLM - INFO - selected model name for cost calculation: ollama/llama3.1
2025-06-06 14:22:20,435 - LiteLLM - INFO - selected model name for cost calculation: ollama/llama3.1
2025-06-06 14:22:20,456 - kombu.connection - WARNING - No hostname was supplied. Reverting to default 'localhost'
2025-06-06 14:23:20,441 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.1; provider = ollama
2025-06-06 14:23:22,830 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-06 14:23:22,836 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-06-06 14:23:22,837 - LiteLLM - INFO - selected model name for cost calculation: ollama/llama3.1
2025-06-06 14:23:22,837 - LiteLLM - INFO - selected model name for cost calculation: ollama/llama3.1
2025-06-06 14:23:22,839 - LiteLLM - INFO - selected model name for cost calculation: ollama/llama3.1
2025-06-06 14:24:22,840 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.1; provider = ollama
2025-06-06 14:24:24,718 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-06 14:24:24,723 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-06-06 14:24:24,725 - LiteLLM - INFO - selected model name for cost calculation: ollama/llama3.1
2025-06-06 14:24:24,725 - LiteLLM - INFO - selected model name for cost calculation: ollama/llama3.1
2025-06-06 14:24:24,733 - LiteLLM - INFO - selected model name for cost calculation: ollama/llama3.1
2025-06-06 14:25:24,738 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.1; provider = ollama
2025-06-06 14:25:29,926 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-06 14:25:29,937 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-06-06 14:25:29,943 - LiteLLM - INFO - selected model name for cost calculation: ollama/llama3.1
2025-06-06 14:25:29,944 - LiteLLM - INFO - selected model name for cost calculation: ollama/llama3.1
2025-06-06 14:25:29,948 - LiteLLM - INFO - selected model name for cost calculation: ollama/llama3.1
